{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型思路：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sys,random\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "\n",
    "os.mkdir('featurescore')\n",
    "os.mkdir('model')\n",
    "os.mkdir('preds')\n",
    "\n",
    "\n",
    "\n",
    "#离散化特征的计数特征\n",
    "test_nd = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/test_x_nd.csv')[['uid','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10']]\n",
    "train_nd = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/train_x_nd.csv')[['uid','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10']]\n",
    "trainunlabeled_nd = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/train_unlabeled_nd.csv')[['uid','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10']]\n",
    "\n",
    "#缺失值个数的离散化特征\n",
    "test_dnull = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/test_x_null.csv')[['uid','discret_null']]\n",
    "train_dnull = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/train_x_null.csv')[['uid','discret_null']]\n",
    "trainunlabeled_dnull = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/train_unlabeled_null.csv')[['uid','discret_null']]\n",
    "\n",
    "#n1~n10，discret_null 这11维特征不做特征选择，先放在一起\n",
    "eleven_feature = ['n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','discret_null']\n",
    "test_eleven = pd.merge(test_nd,test_dnull,on='uid')\n",
    "train_eleven = pd.merge(train_nd,train_dnull,on='uid')\n",
    "trainunlabeled_eleven = pd.merge(trainunlabeled_nd,trainunlabeled_dnull,on='uid')\n",
    "\n",
    "del test_dnull,train_dnull,trainunlabeled_dnull\n",
    "del test_nd,train_nd,trainunlabeled_nd\n",
    "\n",
    "\n",
    "#离散特征\n",
    "discret_feature_score = pd.read_csv('./discret_feature_score.csv')\n",
    "#取前500个\n",
    "fs = list(discret_feature_score.feature[0:500])\n",
    "discret_train = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_x_discretization.csv\")[['uid']+fs]\n",
    "discret_test = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/test_x_discretization.csv\")[['uid']+fs]\n",
    "discret_train_unlabeled = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_unlabeled_discretization.csv\")[['uid']+fs]\n",
    "\n",
    "#排序特征\n",
    "rank_feature_score = pd.read_csv('./rank_feature_score.csv')\n",
    "#取前500个\n",
    "fs = list(rank_feature_score.feature[0:500])\n",
    "rank_train_x = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_x_rank.csv\")\n",
    "rank_train = rank_train_x[fs] / float(len(rank_train_x))\n",
    "rank_train['uid'] = rank_train_x.uid\n",
    "\n",
    "rank_test_x = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/test_x_rank.csv\")\n",
    "rank_test = rank_test_x[fs] / float(len(rank_test_x))\n",
    "rank_test['uid'] = rank_test_x.uid\n",
    "\n",
    "rank_train_unlabeled_x = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_unlabeled_rank.csv\")\n",
    "rank_train_unlabeled = rank_train_unlabeled_x[fs] / float(len(rank_train_unlabeled_x))\n",
    "rank_train_unlabeled['uid'] = rank_train_unlabeled_x.uid\n",
    "\n",
    "del rank_train_x,rank_test_x,rank_train_unlabeled_x\n",
    "\n",
    "#原始特征\n",
    "raw_feature_score = pd.read_csv('./raw_feature_score.csv')\n",
    "#取前500个\n",
    "fs = list(raw_feature_score.feature[0:500])\n",
    "raw_train_x = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_x.csv\")[['uid']+fs]\n",
    "raw_train_y = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/train_y.csv\")\n",
    "raw_train = pd.merge(raw_train_x,raw_train_y,on='uid')\n",
    "del raw_train_x,raw_train_y\n",
    "\n",
    "raw_test = pd.read_csv(\"C:/Users/tianjiayang/loan_predicting/data/test_x.csv\")[['uid']+fs]\n",
    "raw_train_unlabel = pd.read_csv('C:/Users/tianjiayang/loan_predicting/data/train_unlabeled.csv')[['uid']+fs]\n",
    "\n",
    "#将原始特征，排序特征，离散特征，以及其他11维特征（n1～n10，discret_null）合并\n",
    "train = pd.merge(raw_train,rank_train,on='uid')\n",
    "train = pd.merge(train,discret_train,on='uid')\n",
    "train = pd.merge(train,train_eleven,on='uid')\n",
    "\n",
    "test = pd.merge(raw_test,rank_test,on='uid')\n",
    "test = pd.merge(test,discret_test,on='uid')\n",
    "test = pd.merge(test,test_eleven,on='uid')\n",
    "test_uid = test.uid\n",
    "\n",
    "\n",
    "\n",
    "#从无标签数据里面选取部分样本作为负样本。\n",
    "#不想加这部分数据的话，把0.16设置为0,不加这部分数据auc也能到724，加了会有提升\n",
    "xgb717_predict_unlabeled_data = pd.read_csv('./xgb717_predict_unlabeled_data.csv')\n",
    "unlabeldata_0 = xgb717_predict_unlabeled_data[xgb717_predict_unlabeled_data.score<0.16]  #2672个\n",
    "\n",
    "tmp = pd.merge(unlabeldata_0,raw_train_unlabel,on=\"uid\",how=\"left\")\n",
    "tmp1 = pd.merge(tmp,rank_train_unlabeled,on=\"uid\",how=\"left\")\n",
    "tmp2 = pd.merge(tmp1,trainunlabeled_eleven,on=\"uid\",how=\"left\")\n",
    "neg_sample = pd.merge(tmp2,discret_train_unlabeled,on=\"uid\",how=\"left\")\n",
    "neg_sample = neg_sample.drop([\"score\",\"uid\"],axis=1)\n",
    "neg_sample['y'] = [0 for _ in range(len(neg_sample))]\n",
    "\n",
    "print (\"select {0} negative sample from train_unlabel.csv\".format(len(neg_sample)))\n",
    "del unlabeldata_0,tmp,tmp1,tmp2\n",
    "\n",
    "#将缺失值个数在区间5（即缺失值个数大于194的）的样本去掉。这个对结果的提升很大，从0.723提高到接近0.725\n",
    "train = train[train.discret_null!=5]\n",
    "neg_sample = neg_sample[neg_sample.discret_null!=5]\n",
    "\n",
    "\n",
    "def pipeline(iteration,random_seed,feature_num,rank_feature_num,discret_feature_num,gamma,max_depth,lambd,subsample,colsample_bytree,min_child_weight):\n",
    "    #选取的前n个原始特征、排序特征、离散特征\n",
    "    raw_feature_selected = list(raw_feature_score.feature[0:feature_num])\n",
    "    rank_feature_selected = list(rank_feature_score.feature[0:rank_feature_num])\n",
    "    discret_feature_selected = list(discret_feature_score.feature[0:discret_feature_num])\n",
    "\n",
    "    #根据选取的特征构造出训练集，测试集，以及从无标签数据中获取的负样本\n",
    "    train_xy = train[eleven_feature+raw_feature_selected+rank_feature_selected+discret_feature_selected+['y']]\n",
    "    train_xy[train_xy<0] = -1    #缺失值-1或-2，都统一对待，设置为-1\n",
    "\n",
    "    test_x = test[eleven_feature+raw_feature_selected+rank_feature_selected+discret_feature_selected]\n",
    "    test_x[test_x<0] = -1\n",
    "\n",
    "    neg = neg_sample[eleven_feature+raw_feature_selected+rank_feature_selected+discret_feature_selected+['y']]\n",
    "    neg[neg<0] = -1   \n",
    "    \n",
    "    #将从无标签数据中选取出的负样本和原始训练数据合并\n",
    "    train_xy = pd.concat([train_xy,neg])\n",
    "    y = train_xy.y\n",
    "    X = train_xy.drop(['y'],axis=1)\n",
    "    \n",
    "    #xgboost start\n",
    "    dtest = xgb.DMatrix(test_x)\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "    params={\n",
    "    \t'booster':'gbtree',\n",
    "    \t'objective': 'binary:logistic',\n",
    "    \t'scale_pos_weight': float(len(y)-sum(y))/float(sum(y)),\n",
    "        'eval_metric': 'auc',\n",
    "    \t'gamma':gamma,\n",
    "    \t'max_depth':max_depth,\n",
    "    \t'lambda':lambd,\n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        'min_child_weight':min_child_weight, \n",
    "        'eta': 0.08,\n",
    "    \t'seed':random_seed,\n",
    "    \t'nthread':8\n",
    "        }\n",
    "    \n",
    "    watchlist  = [(dtrain,'train')]\n",
    "    model = xgb.train(params,dtrain,num_boost_round=1500,evals=watchlist)\n",
    "    model.save_model('./model/xgb{0}.model'.format(iteration))\n",
    "    \n",
    "    #predict test set\n",
    "    test_y = model.predict(dtest)\n",
    "    test_result = pd.DataFrame(test_uid,columns=[\"uid\"])\n",
    "    test_result[\"score\"] = test_y\n",
    "    test_result.to_csv(\"./preds/xgb{0}.csv\".format(iteration),index=None,encoding='utf-8')\n",
    "    \n",
    "    #save feature score\n",
    "    feature_score = model.get_fscore()\n",
    "    feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)\n",
    "    fs = []\n",
    "    for (key,value) in feature_score:\n",
    "        fs.append(\"{0},{1}\\n\".format(key,value))\n",
    "    \n",
    "    with open('./featurescore/feature_score_{0}.csv'.format(iteration),'w') as f:\n",
    "        f.writelines(\"feature,score\\n\")\n",
    "        f.writelines(fs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    random_seed = range(1000,2000,10)\n",
    "    feature_num = range(300,500,2)\n",
    "    rank_feature_num = range(300,500,2)\n",
    "    discret_feature_num = range(64,100,1)\n",
    "    gamma = [i/1000.0 for i in range(0,300,3)]\n",
    "    max_depth = [6,7,8]\n",
    "    lambd = range(500,700,2)\n",
    "    subsample = [i/1000.0 for i in range(500,700,2)]\n",
    "    colsample_bytree = [i/1000.0 for i in range(250,350,1)]\n",
    "    min_child_weight = [i/1000.0 for i in range(250,550,3)]\n",
    "    random.shuffle(rank_feature_num)\n",
    "    random.shuffle(random_seed)\n",
    "    random.shuffle(feature_num)\n",
    "    random.shuffle(discret_feature_num)\n",
    "    random.shuffle(gamma)\n",
    "    random.shuffle(max_depth)\n",
    "    random.shuffle(lambd)\n",
    "    random.shuffle(subsample)\n",
    "    random.shuffle(colsample_bytree)\n",
    "    random.shuffle(min_child_weight)\n",
    "    \n",
    "    with open('params.pkl','w') as f:\n",
    "        cPickle.dump((random_seed,feature_num,rank_feature_num,discret_feature_num,gamma,max_depth,lambd,subsample,colsample_bytree,min_child_weight),f)\n",
    "    \"\"\"\n",
    "\n",
    "    with open('params_for_reproducing.pkl','rb') as f:\n",
    "        random_seed,feature_num,rank_feature_num,discret_feature_num,gamma,max_depth,lambd,subsample,colsample_bytree,min_child_weight = cPickle.load(f)\n",
    "    \n",
    "    \n",
    "    for i in range(36):\n",
    "        print (\"iter:{}\".format(i))\n",
    "        pipeline(i,random_seed[i],feature_num[i],rank_feature_num[i],discret_feature_num[i],gamma[i],max_depth[i%3],lambd[i],subsample[i],colsample_bytree[i],min_child_weight[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute average of each model's output\n",
    "\n",
    "#coding=utf-8\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "files = os.listdir('./preds')\n",
    "pred = pd.read_csv('./preds/'+files[0])\n",
    "uid = pred.uid\n",
    "score = pred.score\n",
    "for f in files[1:]:\n",
    "    pred = pd.read_csv('./preds/'+f)\n",
    "    score += pred.score\n",
    "\n",
    "score /= len(files)\n",
    "\n",
    "pred = pd.DataFrame(uid,columns=['uid'])\n",
    "pred['score'] = score\n",
    "pred.to_csv('avg_preds.csv',index=None,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
